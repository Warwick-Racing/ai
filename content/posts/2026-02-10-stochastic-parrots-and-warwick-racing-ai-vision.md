---
title: "From Stochastic Parrots to a Better Brain for the Driver"
date: "2026-02-23"
author: "Perry Hui"
tags: ["ai", "vision"]
summary: "A reflection on intelligence, neural networks, and Warwick Racing AI's vision to build autonomous machine intelligenceâ€”not just a faster car, but a better brain for the driver."
---

We have all been there: It's 3AM, the cursor is mockingly blinking, and the looming deadline feels like a hanging knife about our heads. Enter "the answer to everything", the online chatbot (Gemini, ChatGPT etc) that turns a frantic prompt into a sensible response. While most of us have become experts at using Artificial Intelligence (AI) to save our skin (and sleep), few of us actually know what's happening behind the screen. Is it a digital brain, or a mere "stochastic parrot"?

Before we dive deep into the current AI landscape, let's digress to a discussion of intelligence. Indeed, it is rather futile to solve a problem without having defined it properly in the first place. Intelligence, stems from the Latin verb intelligere: to comprehend/ perceive. An incalculable number of neuroscientists, biologists and philosophers have come up with definitions to the concept and the conditions on how intelligence can be achieved but have failed to reach a unified consensus.

It is a human tendency to "Play God" and that is what made researchers look inwards for inspirations to mimicking intelligence. The idea of mimicking biological neurons with mathematical functions was first proposed in 1943 (actually not that long ago) by Warren McCulloch and Walter Pitt's A Logical Calculus of the Ideas Immanent in Nervous Activity. At each neuron, inputs are averaged against their channel weights and fed through a threshold function (Heaviside step function) that outputs 0 if the aggregation is below the set threshold and 1 otherwise. It is worth noting that the individual neuron weights are fixed and that the whole network can be interpreted as compositions of Boolean Functions. This work was later adapted by Frank Rosenblatt as the Perceptron Algorithm, instead allowing for continuous real valued in/ outputs and weight variation (learning). It is worth noting that a unique combination of threshold function and loss function applied on the Perceptron Algorithm defines a class of linear learning algorithms.

Researchers realised that stacking multiple "Perceptron Algorithms" (each unit is regarded as a neuron) into layers, with each "pathway" i.e. information traversing through sequentially ordered neurons with each being viewed as a differentiable mathematical function, meant that the network is capable of "learning" more complex patterns. The "learning" was done through "minimising" the error at each neuron across all possible pathways which in turn minimises the systematic error of the network (Backpropagation). These networks, short for Artificial Neural Networks, have been at the heart of many breakthroughs in computer vision, translation among others.

Geoff Hinton, Alex Krizhevsky and Ilya Sutskever, in 2012, achieved top 5 in the ImageNet competition with AlexNet (a Convolutional Neural Network trained on an Nvidia GPU by randomly "switching off" connections between neurons). It was indeed a breakthrough at the time, as they have proved that neural networks are capable of outranking traditional statistics-based learning methods. This is otherwise known as the Deep Learning Revolution, where the following decade was filled with innovations from improving Convolutional Neural Networks, the transformer architecture that powers state of the art chatbots to the manifold hypothesis that alters the fundamental understanding of the space that data is drawn from.

I shall give my own interpretation to the subject: Intelligence is the culmination of creativity, curiosity, abstraction of logic, metacognition and functional reactivity. Having the ability to imitate speech does not automatically guarantee intelligence. The current AI industry largely believes that achieving human-level intelligence can be achieved through scaling computations and you have probably heard of the absurdly expensive data centre deals coming out of Nvidia and other computer manufacturers. This is exactly why my workstation is so damn expensive (Can we please get some sponsors?).

Language is a distillation of the world through human experience and is systematically biased: AIs are trained on an interpretation of the world and indeed increasingly through synthetic language generated from themselves. They do not, themselves, possess their own fundamental perception and mental model of the world that we live in. They remain stochastic parrots: Brilliant at predicting the next word, but hollow in their understanding of the world's physical truth.

There is another way. Prof. Fei-fei Li and Yann Lecun have largely swerved away from minimising the systematic error of the network, rather they try to ensure that the AI can perceive representations of concepts and react towards them. This is known as world models and LeCun's JEPA (Joint Embedding Prediction Architecture) is a step towards this end.

As one of the newer contenders on the AI grid, our debut at Silverstone isn't just a race: It is the forging of our bedrock. Building a Minimum Viable Product that prioritises reliability for Silverstone has been our paramount focus this year at Warwick Racing AI. However, it is our vision that, in the coming years, the team moves beyond the current standards of reactive computing. We should not be merely training our AI to follow a path; we are engineering the discipline of autonomous Machine Intelligence (LeCun 2022): Our platform should be able to perceive its environment, identify inconsistencies from sensor inputs with its internal representation and react accordingly.

At Warwick Racing AI, we aren't merely building a faster car; we are building a better brain for the driver.
